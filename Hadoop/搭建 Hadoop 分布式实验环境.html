<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/static/css/typo.css">
        <link rel="Stylesheet" type="text/css" href="/static/css/reset_typo.css">
        <link rel="Stylesheet" type="text/css" href="/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/static/css/style.css">
        <link rel="shortcut icon" href="/static/images/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/static/images/favicon.ico" type="image/x-icon">
        <title>搭建 Hadoop 分布式实验环境 - Wiki | Small Cpp</title>
        <meta name="keywords" content=""/>
        <meta name="description" content=""/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body>
        <div id="container" class="typo">
            
    <div id="header">
        <div id="post-nav">
            
            <a href="/">Home</a> » <a href="/#Hadoop">Hadoop</a> » 搭建 Hadoop 分布式实验环境
            
        </div>
    </div>
    <div class="clearfix"></div>
    <div id="content">
        <p><strong>实验最终成品</strong>:</p>
<ul>
<li>宿主机: win7 64位, 16G</li>
<li>虚拟化工具: VMware Workstation</li>
<li>虚拟机系统: Ubuntu 32位, 1.5G, 20G, NAT</li>
<li>主机名: smallcpp01 (NameNode), smallcpp02 (Datanode), smallcpp03 (Datanode)</li>
<li>Java 版本: jdk-8u101-linux-i586</li>
<li>Hadoop 版本: hadoop-2.7.3</li>
</ul>
<p>关于虚拟机的网络模式解释下, 推荐的是使用<strong>桥接</strong>模式, 因为桥接模式可以让虚拟机和宿主机处于同一网段, 而 <strong>NAT</strong> 模式则是虚拟机单独分配一个网段;<br>然而我实验时发现<strong>桥接</strong>模式虚拟机能 ping 通局域网内所有其他 ip 及外网 ip, 但就是不能和宿主机互 ping, 搞了好久没搞定, 万般无奈选择了 <strong>NAT</strong> 模式.</p>
<p>虚拟机的安装参考另一篇 wiki: <a href="http://wiki.smallcpp.cn/%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/VMware%20%E5%AE%89%E8%A3%85%20Ubuntu.html">使用 VMware 安装 Ubuntu  、VM Tools 和 Fcitx 输入法</a></p>
<p>安装好后虚拟机后, 开始搭建 Hadoop 分布式实验环境.</p>
<div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1-iphosts-iptables">1. 固定 IP、主机名、Hosts 和 iptables</a><ul>
<li><a href="#ip">固定 IP</a></li>
<li><a href="#dns">DNS</a></li>
<li><a href="#hosts">Hosts</a></li>
<li><a href="#_1">关闭防火墙</a></li>
</ul>
</li>
<li><a href="#2-jdk">2. 安装 JDK</a><ul>
<li><a href="#21">2.1</a></li>
<li><a href="#22">2.2</a></li>
<li><a href="#23">2.3</a></li>
</ul>
</li>
<li><a href="#3-hadoop">3. 安装 Hadoop</a><ul>
<li><a href="#31">3.1</a></li>
<li><a href="#32">3.2</a></li>
<li><a href="#33">3.3</a></li>
</ul>
</li>
<li><a href="#4-hadoop">4. 配置 Hadoop</a><ul>
<li><a href="#41-vim-hadoop-envsh">4.1. vim hadoop-env.sh</a></li>
<li><a href="#42-vim-core-sitexml">4.2. vim core-site.xml</a></li>
<li><a href="#43-vim-hdfs-sitexml">4.3. vim hdfs-site.xml</a></li>
<li><a href="#44-vim-yarn-envsh">4.4. vim yarn-env.sh</a></li>
<li><a href="#45-vim-yarn-sitexml">4.5. vim yarn-site.xml</a></li>
<li><a href="#46-mapred-sitexml">4.6. mapred-site.xml</a></li>
<li><a href="#47-vim-slaves">4.7. vim slaves</a></li>
<li><a href="#48">4.8. 克隆虚拟机</a></li>
</ul>
</li>
<li><a href="#5-ssh">5. 配置 SSH 免密码登录</a><ul>
<li><a href="#51-smallcpp01">5.1. 配置 smallcpp01</a></li>
<li><a href="#52-smallcpp02-smallcpp03">5.2. 配置 smallcpp02 和 smallcpp03</a></li>
</ul>
</li>
<li><a href="#6">6. 启动集群</a></li>
<li><a href="#7">7. 测试集群</a><ul>
<li><a href="#71">7.1. 上传文件</a></li>
<li><a href="#72">7.2. 下载文件</a></li>
<li><a href="#73-mr">7.3. 测试 MR 作业</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
</div>
<h1 id="1-iphosts-iptables">1. 固定 IP、主机名、Hosts 和 iptables</h1>
<h2 id="ip">固定 IP</h2>
<p><strong>sudo vim /etc/network/interfaces</strong>, 编辑 interfaces 文件, 这是 Ubuntu 网上配置文件.</p>
<div class="hlcode"><pre><span class="k">auto</span> <span class="n">lo</span>
<span class="n">iface</span> <span class="n">lo</span> <span class="n">inet</span> <span class="n">loopback</span>

<span class="k">auto</span> <span class="n">eth0</span>
<span class="n">iface</span> <span class="n">eth0</span> <span class="n">inet</span> <span class="k">static</span>
<span class="n">address</span> <span class="mf">192.168.31.200</span>
<span class="n">netmask</span> <span class="mf">255.255.255.0</span>
<span class="n">gateway</span> <span class="mf">192.168.31.2</span>
</pre></div>


<p><br><br />
初始文件只有前面两行, 后面的是要添加的内容.</p>
<ul>
<li>第 <strong>1</strong> 行跟第 <strong>4</strong> 行说明 <code>lo</code> 接口跟 <code>eth0</code> 接口会在系统启动时被自动配置;</li>
<li>第 <strong>2</strong> 行将 <code>lo</code> 接口设置为一个本地回环 (loopback) 地址;</li>
<li>第 <strong>5</strong> 行指出 <code>eth0</code> 接口具有一个静态的 (static) IP 配置;</li>
<li>第 <strong>6</strong> 行-第 <strong>8</strong> 行分别设置 <code>eth0</code> 接口的 ip、掩码和网关.</li>
</ul>
<p>但是需要注意, 并不所有的都是 <code>eth0</code>, 采用的 Ubuntu 版本不同, 也有可能是其他的接口, 如 <code>ens32</code>; 可以先使用 <code>ifconfig</code> 看下系统使用的是哪个接口.</p>
<p><img alt="" src="http://wiki.smallcpp.cn/static/images/搭建Hadoop分布式实验环境/ifconfig.png" /></p>
<p>另外就是 <strong>gateway</strong> (网关), 虚拟机的网关可以通过虚拟网络编辑器查看.</p>
<p><img alt="" src="http://wiki.smallcpp.cn/static/images/搭建Hadoop分布式实验环境/虚拟网络编辑器.png" /></p>
<p><img alt="" src="http://wiki.smallcpp.cn/static/images/搭建Hadoop分布式实验环境/NAT.png" /></p>
<p><img alt="" src="http://wiki.smallcpp.cn/static/images/搭建Hadoop分布式实验环境/gateway.png" /></p>
<p>所以上面的第八行的 <strong>gateway</strong> 要填: <code>192.168.142.2</code>.</p>
<h2 id="dns">DNS</h2>
<p><strong>sudo vim /etc/resolv.conf</strong>, 编辑 DNS 解析文件.</p>
<div class="hlcode"><pre><span class="n">nameserver</span> <span class="mf">8.8.8.8</span>
<span class="n">nameserver</span> <span class="mf">8.8.4.4</span>
<span class="n">nameserver</span> <span class="mf">192.168.31.2</span>
</pre></div>


<p><br><br />
第 3 行填 <strong>gateway</strong> (网关) ip;</p>
<p>改完上面, 如果重启的话, DNS 还是会变为原来的样子;</p>
<p>网上给出的方法是执行 <code>sudo vim /etc/resolvconf/resolv.conf.d/base</code> 输入和 DNS 解析文件相同的内容.</p>
<p>实际操作后重启发现好像并没生效, 又找到了另一个方法, 执行 <code>sudo vim /etc/resolvconf/resolv.conf.d/tail</code> 输入和 DNS 解析文件相同的内容.</p>
<p>最后, 通过 <code>sudo resolvconf -u</code> 刷新 <strong>resolv.conf</strong> 文件, 再用 <code>sudo /etc/init.d/networking restart</code> 重启网络.</p>
<h2 id="hosts">Hosts</h2>
<p><code>sudo vim /etc/hostname</code> 修改主机名为 smallcpp01 (另外两台分别用 smallcpp02 和 smallcpp03).</p>
<p><code>sudo vim /etc/hosts</code> 修改 [ip 域名] 对应表.</p>
<div class="hlcode"><pre><span class="mf">127.0.0.1</span>       <span class="n">localhost</span>
<span class="mf">192.168.31.200</span>  <span class="n">smallcpp01</span>
<span class="mf">192.168.31.201</span>  <span class="n">smallcpp02</span>
<span class="mf">192.168.31.201</span>  <span class="n">smallcpp03</span>
</pre></div>


<p><br></p>
<h2 id="_1">关闭防火墙</h2>
<p>iptables 是 linux 下一个简单实用的防火墙组件.</p>
<p>先用 <code>sudo ufw status</code> 查看防火墙状态, 如果是启用的, 就用 <code>sudo ufw disable</code> 关闭它.</p>
<blockquote>
<p>Ubuntu 默认不安装 selinux</p>
</blockquote>
<p>都设置好后, <code>reboot</code> 重启系统, <code>ifconfig</code> 查看 ip 是否变为我们设置的, <code>hostname</code> 查看主机名.</p>
<h1 id="2-jdk">2. 安装 JDK</h1>
<h2 id="21">2.1</h2>
<p>访问 oracle 官网: <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p>
<p>我这里选择的是最新的 <code>jdk-8u101-linux-i586</code>.</p>
<p>下载好后, 弄到 Ubuntu 里解压, 然后在 <code>usr</code> 目录下新建一个 <code>java</code> 目录, 把解压好的文件复制进去.</p>
<div class="hlcode"><pre><span class="n">sudo</span> <span class="n">mkdir</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">java</span>
<span class="n">sudo</span> <span class="n">mv</span> <span class="err">桌面</span><span class="o">/</span><span class="n">jdk1</span><span class="mf">.8.0</span><span class="n">_101</span><span class="o">/</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">java</span><span class="o">/</span>
</pre></div>


<p><br><br />
或者可以先 <code>sudo mkdir /usr/java</code> 创建好目录, 再用 <code>sudo tar -zxvf jdk-8u101-linux-i586.tar.gz -C /usr/java</code> (-z 处理 gzip, x 解压, v 显示详情, f 解压哪个文件) 直接解压到 <code>/usr/java</code> 下.</p>
<h2 id="22">2.2</h2>
<p>修改环境变量.</p>
<p><code>vim ~/.bashrc</code> 打开 VIM 编辑器后, 翻到最后一行, 在后面添加:</p>
<div class="hlcode"><pre><span class="n">export</span> <span class="n">JAVA_HOME</span><span class="o">=/</span><span class="n">usr</span><span class="o">/</span><span class="n">java</span><span class="o">/</span><span class="n">jdk1</span><span class="mf">.8.0</span><span class="n">_101</span>
<span class="n">export</span> <span class="n">PATH</span><span class="o">=</span><span class="err">$</span><span class="n">PATH</span><span class="o">:</span><span class="err">$</span><span class="n">JAVA_HOME</span><span class="o">/</span><span class="n">bin</span>
</pre></div>


<p><br><br />
注意, "=" 左右两边不能有空格; 最后刷新下文件.</p>
<h2 id="23">2.3</h2>
<p>刷新环境变量.</p>
<div class="hlcode"><pre><span class="n">source</span> <span class="o">~/</span><span class="p">.</span><span class="n">bashrc</span>
</pre></div>


<p><br><br />
此时, 不管在哪个目录输入 <code>java -version</code> 都可以找到执行文件.</p>
<p><img alt="" src="http://wiki.smallcpp.cn/static/images/搭建Hadoop分布式实验环境/javaversion.png" /></p>
<h1 id="3-hadoop">3. 安装 Hadoop</h1>
<h2 id="31">3.1</h2>
<p>访问: <a href="http://archive.apache.org/dist/">http://archive.apache.org/dist/</a>, apache 的所有项目都在这里.</p>
<p><img alt="" src="http://i61.tinypic.com/29ustjt.jpg" /></p>
<p><img alt="" src="http://i60.tinypic.com/33xy72x.jpg" /></p>
<p><img alt="" src="http://i58.tinypic.com/2nuon4o.jpg" /></p>
<p><img alt="" src="http://wiki.smallcpp.cn/static/images/搭建Hadoop分布式实验环境/hadoopdown.png" /></p>
<p>下载完成后, 拖到 Ubuntu 桌面.</p>
<p><code>sudo mkdir /usr/smallcpp</code> 创建一个文件夹.</p>
<p><code>cd ~/桌面</code>, 进入桌面目录.</p>
<p><code>sudo tar -zxvf hadoop-2.7.3.tar.gz -C /usr/smallcpp</code> (-z 处理 gzip, x 解压, v 显示详情, f 解压哪个文件)</p>
<p>为避免权限问题, 可将 <code>/usr/smallcpp/hadoop-2.7.3/</code> 目录权限改为 <strong>777</strong>: <code>sudo chmod -R 777 /usr/smallcpp/hadoop-2.7.3/</code></p>
<h2 id="32">3.2</h2>
<p>修改环境变量.</p>
<p><code>vim ~/.bashrc</code></p>
<div class="hlcode"><pre><span class="n">export</span> <span class="n">JAVA_HOME</span><span class="o">=/</span><span class="n">usr</span><span class="o">/</span><span class="n">java</span><span class="o">/</span><span class="n">jdk1</span><span class="mf">.8.0</span><span class="n">_101</span>
<span class="n">export</span> <span class="n">HADOOP_HOME</span><span class="o">=/</span><span class="n">usr</span><span class="o">/</span><span class="n">smallcpp</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="mf">2.7.3</span>
<span class="n">export</span> <span class="n">PATH</span><span class="o">=</span><span class="err">$</span><span class="n">PATH</span><span class="o">:</span><span class="err">$</span><span class="n">JAVA_HOME</span><span class="o">/</span><span class="n">bin</span><span class="o">:</span><span class="err">$</span><span class="n">HADOOP_HOME</span><span class="o">/</span><span class="n">bin</span><span class="o">:</span><span class="err">$</span><span class="n">HADOOP_HOME</span><span class="o">/</span><span class="n">sbin</span>
</pre></div>


<p><br></p>
<h2 id="33">3.3</h2>
<p>刷新环境变量.</p>
<p><code>source ~/.bashrc</code> 退回根目录, 测试下 hadoop 命令: <code>hadoop version</code></p>
<p><img alt="" src="http://wiki.smallcpp.cn/static/images/搭建Hadoop分布式实验环境/hadoopversion.png" /></p>
<h1 id="4-hadoop">4. 配置 Hadoop</h1>
<p><code>cd /usr/smallcpp/hadoop-2.7.3/etc/hadoop</code> 进入 Hadoop 配置文件所在目录.</p>
<h2 id="41-vim-hadoop-envsh">4.1. vim hadoop-env.sh</h2>
<p>定位到 26 行左右, 找到</p>
<div class="hlcode"><pre>export JAVA_HOME=<span class="cp">${</span><span class="n">JAVA_HOME</span><span class="cp">}</span>
</pre></div>


<p><br><br />
改成:</p>
<div class="hlcode"><pre><span class="n">export</span> <span class="n">JAVA_HOME</span><span class="o">=/</span><span class="n">usr</span><span class="o">/</span><span class="n">java</span><span class="o">/</span><span class="n">jdk1</span><span class="mf">.8.0</span><span class="n">_101</span> <span class="p">(</span><span class="err">可以在</span> <span class="n">vim</span> <span class="err">的命令模式下</span><span class="p">,</span> <span class="err">通过</span> <span class="n">echo</span> <span class="err">$</span><span class="n">JAVA_HOME</span> <span class="err">查看路径</span><span class="p">)</span>
</pre></div>


<p><br></p>
<h2 id="42-vim-core-sitexml">4.2. vim core-site.xml</h2>
<div class="hlcode"><pre><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
        <span class="c">&lt;!--用来指定 HDFS 的老大(NameNode)的地址--&gt;</span>
                <span class="nt">&lt;name&gt;</span>fs.defaultFS<span class="nt">&lt;/name&gt;</span>
                <span class="c">&lt;!--smallcpp01 是这台主机名, 要在 hosts 里设置了映射才可以, 不然只能写 ip--&gt;</span>
                <span class="nt">&lt;value&gt;</span>hdfs://smallcpp01:9000<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>

        <span class="nt">&lt;property&gt;</span>
        <span class="c">&lt;!--用来指定 hadoop 运行时产生文件的存放目录--&gt;</span>
        <span class="c">&lt;!--默认为系统目录, 重启会被清空, 导致重启 hadoop 不能用--&gt;</span>
                <span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>/usr/smallcpp/hadoop-2.7.3/tmp<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</pre></div>


<h2 id="43-vim-hdfs-sitexml">4.3. vim hdfs-site.xml</h2>
<div class="hlcode"><pre><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
        <span class="c">&lt;!--指定 HDFS 保存数据的副本个数, 一般最大为 3 就差不多了--&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>2<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
    <span class="c">&lt;!--指定元数据保存目录--&gt;</span>
             <span class="nt">&lt;name&gt;</span>dfs.namenode.name.dir<span class="nt">&lt;/name&gt;</span>
             <span class="nt">&lt;value&gt;</span>/usr/smallcpp/hadoop-2.7.3/tmp/dfs/name<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
    <span class="c">&lt;!--指定 HDFS 保存数据目录--&gt;</span>
             <span class="nt">&lt;name&gt;</span>dfs.datanode.data.dir<span class="nt">&lt;/name&gt;</span>
             <span class="nt">&lt;value&gt;</span>/usr/smallcpp/hadoop-2.7.3/tmp/dfs/data<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</pre></div>


<p><br><br />
我们这里有 smallcpp02 和 smallcpp03 两台数据节点, 所以 <code>dfs.replication</code> 为 2, 如果是伪分布式系统的话, 这里改为 1 就可以了.</p>
<h2 id="44-vim-yarn-envsh">4.4. vim yarn-env.sh</h2>
<p>定位到 23 行左右, 找到 JAVA_HOME, 改为 <code>export JAVA_HOME=/usr/java/jdk1.8.0_101</code></p>
<h2 id="45-vim-yarn-sitexml">4.5. vim yarn-site.xml</h2>
<div class="hlcode"><pre><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
            <span class="c">&lt;!--NodeManager 获取数据的方式是shuffle--&gt;</span>
                <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
            <span class="c">&lt;!--指定 YARN 的老大(ResourceManager 它负责资源的调度、分配)的地址--&gt;</span>
                <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.hostname<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>smallcpp01<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</pre></div>


<p><br></p>
<h2 id="46-mapred-sitexml">4.6. mapred-site.xml</h2>
<div class="hlcode"><pre><span class="n">cp</span> <span class="n">mapred</span><span class="o">-</span><span class="n">site</span><span class="p">.</span><span class="n">xml</span><span class="p">.</span><span class="n">template</span> <span class="n">mapred</span><span class="o">-</span><span class="n">site</span><span class="p">.</span><span class="n">xml</span>
<span class="n">vim</span> <span class="n">mapred</span><span class="o">-</span><span class="n">site</span><span class="p">.</span><span class="n">xml</span>
</pre></div>


<p><br></p>
<div class="hlcode"><pre><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
        <span class="c">&lt;!--告诉 Hadoop MR 要运行在 yarn 上--&gt;</span>
                <span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>mapreduce.jobhistory.address<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>smallcpp01:10020<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>mapreduce.jobhistory.webapp.address<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>smallcpp01:19888<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</pre></div>


<p><br></p>
<h2 id="47-vim-slaves">4.7. vim slaves</h2>
<p>如果是伪分布式环境, 不需要配这个环节.</p>
<p><code>vim slaves</code></p>
<p>打开后去掉第一行的 localhost, 将数据节点的域名添加进来</p>
<div class="hlcode"><pre><span class="n">smallcpp02</span>
<span class="n">smallcpp03</span>
</pre></div>


<p><br><br />
<strong>注意</strong>, 数据节点的域名要在 Hosts 文件中解析了才行!</p>
<h2 id="48">4.8. 克隆虚拟机</h2>
<p>关闭当前虚拟机后, 从当前虚拟机上克隆两份.</p>
<p>修改克隆出来的虚拟机的<strong>固定 IP<strong><em>*、主机名</em>* 和 </strong>Hosts</strong>.</p>
<p><code>sudo vim /etc/network/interfaces</code> 修改固定 IP</p>
<p><code>sudo vim /etc/hostname</code> 修改主机名.</p>
<p><code>sudo vim /etc/hosts</code> 修改 [ip 域名] 对应表.</p>
<div class="hlcode"><pre><span class="mf">127.0.0.1</span>       <span class="n">localhost</span>
<span class="mf">192.168.31.200</span>  <span class="n">smallcpp01</span>
<span class="mf">192.168.31.201</span>  <span class="n">smallcpp02</span>
<span class="mf">192.168.31.202</span>  <span class="n">smallcpp03</span>
</pre></div>


<p><br><br />
配好后重启, 在三台虚拟机间互 Ping 测试下.</p>
<h1 id="5-ssh">5. 配置 SSH 免密码登录</h1>
<p>Ubuntu 默认并没有安装 <strong>ssh</strong> 服务, 需要自己手动安装 <strong>openssh-server</strong>, 可以通过 <code>ssh localhost</code> 判断是否安装 ssh 服务; 如果没有安装则通过 <code>sudo apt-get install openssh-server</code> 安装即可.</p>
<h2 id="51-smallcpp01">5.1. 配置 smallcpp01</h2>
<p>在 smallcpp01 上安装好 <strong>ssh</strong> 服务后.</p>
<p><code>cd ~</code> 进入根目录</p>
<p><code>ls -la</code> 查看下当前目录文件, 可以看到有个隐藏的 <code>.ssh</code> 文件夹 (如果没有自己新建个)</p>
<p><code>cd .ssh/</code> 进入 <code>.ssh</code> 目录, <code>ls</code> 一下, 看看该目录下有没有 <code>id_rsa</code>、<code>id_rsa.pub</code> 两个文件, 如果没有, 就用 <code>ssh-keygen -t rsa</code> 生成一对 (一路回车就好).</p>
<p><code>cp id_rsa.pub authorized_keys</code></p>
<h2 id="52-smallcpp02-smallcpp03">5.2. 配置 smallcpp02 和 smallcpp03</h2>
<p>首先也是先安装好 <strong>ssh</strong> 服务生成一对 <code>id_rsa</code>、<code>id_rsa.pub</code> 文件;</p>
<p>然后<strong>不要</strong>执行 <code>cp id_rsa.pub authorized_keys</code>, 而是执行 <code>ssh-copy-id -i ~/.ssh/id_rsa.pub martin@smallcpp01</code> 将公钥追加到 <strong>smallcpp01</strong> 的 <strong>authorized_keys</strong> 中.</p>
<p>操作好后到 smallcpp01 中 <code>vim authorized_keys</code> 可以看到里面已经多出了 smallcpp02 和 smallcpp03 的公钥了.</p>
<p>最后将 <strong>authorized_keys</strong> 远程拷贝到 smallcpp02 和 smallcpp03 中.</p>
<div class="hlcode"><pre><span class="n">scp</span> <span class="n">authorized_keys</span> <span class="n">martin</span><span class="err">@</span><span class="n">smallcpp02</span><span class="o">:/</span><span class="n">home</span><span class="o">/</span><span class="n">martin</span><span class="o">/</span><span class="p">.</span><span class="n">ssh</span><span class="o">/</span><span class="n">authorized_keys</span>
<span class="n">scp</span> <span class="n">authorized_keys</span> <span class="n">martin</span><span class="err">@</span><span class="n">smallcpp03</span><span class="o">:/</span><span class="n">home</span><span class="o">/</span><span class="n">martin</span><span class="o">/</span><span class="p">.</span><span class="n">ssh</span><span class="o">/</span><span class="n">authorized_keys</span>
</pre></div>


<p><br></p>
<h1 id="6">6. 启动集群</h1>
<p>首次启动需要先在 NameNode 节点 (smallcpp01) 执行 NameNode 的格式化:</p>
<div class="hlcode"><pre><span class="n">hdfs</span> <span class="n">namenode</span> <span class="o">-</span><span class="n">format</span>       <span class="err">#</span> <span class="err">首次运行需要执行初始化，之后不需要</span>
</pre></div>


<p><br><br />
格式化成功后就可以通过以下命令启动集群了:</p>
<div class="hlcode"><pre><span class="p">.</span><span class="o">/</span><span class="n">start</span><span class="o">-</span><span class="n">dfs</span><span class="p">.</span><span class="n">sh</span>
<span class="p">.</span><span class="o">/</span><span class="n">start</span><span class="o">-</span><span class="n">yarn</span><span class="p">.</span><span class="n">sh</span>
<span class="p">.</span><span class="o">/</span><span class="n">mr</span><span class="o">-</span><span class="n">jobhistory</span><span class="o">-</span><span class="n">daemon</span><span class="p">.</span><span class="n">sh</span> <span class="n">start</span> <span class="n">historyserver</span>
</pre></div>


<p><br><br />
或者:</p>
<div class="hlcode"><pre><span class="p">.</span><span class="o">/</span><span class="n">start</span><span class="o">-</span><span class="n">all</span><span class="p">.</span><span class="n">sh</span>
<span class="p">.</span><span class="o">/</span><span class="n">mr</span><span class="o">-</span><span class="n">jobhistory</span><span class="o">-</span><span class="n">daemon</span><span class="p">.</span><span class="n">sh</span> <span class="n">start</span> <span class="n">historyserver</span>
</pre></div>


<p><br><br />
集群成功启动后可以在终端用 <code>JPS</code> 查看当前有哪些 Java 进程, NameNode 节点上应该有以下进程:</p>
<div class="hlcode"><pre><span class="n">NameNode</span>
<span class="n">SecondaryNameNode</span>
<span class="n">ResourceManager</span>
<span class="n">JobHistoryServer</span>
</pre></div>


<p><br><br />
而 DataNode 上应该有以下进程:</p>
<div class="hlcode"><pre><span class="n">DataNode</span>
<span class="n">NodeManager</span>
</pre></div>


<p><br><br />
停止集群命令如下:</p>
<div class="hlcode"><pre><span class="n">stop</span><span class="o">-</span><span class="n">yarn</span><span class="p">.</span><span class="n">sh</span>
<span class="n">stop</span><span class="o">-</span><span class="n">dfs</span><span class="p">.</span><span class="n">sh</span>
<span class="n">mr</span><span class="o">-</span><span class="n">jobhistory</span><span class="o">-</span><span class="n">daemon</span><span class="p">.</span><span class="n">sh</span> <span class="n">stop</span> <span class="n">historyserver</span>
</pre></div>


<p><br><br />
或者:</p>
<div class="hlcode"><pre><span class="n">stop</span><span class="o">-</span><span class="n">all</span><span class="p">.</span><span class="n">sh</span>
<span class="n">mr</span><span class="o">-</span><span class="n">jobhistory</span><span class="o">-</span><span class="n">daemon</span><span class="p">.</span><span class="n">sh</span> <span class="n">stop</span> <span class="n">historyserver</span>
</pre></div>


<p><br><br />
还可以通过 <code>hdfs dfsadmin -report</code> 来查看所有 DataNode 的信息.</p>
<p><img alt="" src="http://wiki.smallcpp.cn/static/images/搭建Hadoop分布式实验环境/dfsadmin.png" /></p>
<p><strong>Live datanodes</strong> 表示当前集群有几个 DataNode 在运行.</p>
<h1 id="7">7. 测试集群</h1>
<p>集群启动成功后会提供 Web 界面来管理集群.</p>
<ul>
<li><a href="http://smallcpp01:50070">smallcpp01:50070</a> -- hdfs 管理界面</li>
<li><a href="http://smallcpp01:8088">smallcpp01:8088</a> -- yarn 管理界面</li>
<li><a href="http://smallcpp01:19888">smallcpp01:19888</a> -- jobhistory 管理界面</li>
</ul>
<p><strong>hdfs 管理界面</strong></p>
<p><img alt="" src="http://i61.tinypic.com/10fcr2s.jpg" /></p>
<p>hadoop fs -ls /<br />
hdfs://smallcpp01:9000</p>
<h2 id="71">7.1. 上传文件</h2>
<p>在 smallcpp01 (不一定是 smallcpp01, 可以集群中的任意一台进行测试) 的桌面上准备了一份大文件, 如 <code>ubuntu-16.04-desktop-amd64.iso</code>, 现在把它上传到 Hadoop 的 HDFS 文件系统上去.</p>
<p><code>hadoop fs -put /home/martin/桌面/ubuntu-16.04-desktop-amd64.iso hdfs://smallcpp01:9000/ubuntu-amd64.iso</code></p>
<p>上传文件到 <code>hdfs://smallcpp01:9000/</code> 并命名为 <code>ubuntu-amd64.iso</code>; 同样功能的命令除了 <code>put</code> 还有 <code>copyFromLocal</code> (过时).</p>
<p><code>hdfs://smallcpp01:9000/</code> 表示的是 HDFS 的<strong>根目录</strong>, 可以简写成 <code>/</code>, 如上面的上传文件命令可以写成这样:</p>
<p><code>hadoop fs -put /home/martin/桌面/ubuntu-16.04-desktop-amd64.iso /ubuntu-amd64.iso</code></p>
<p>Hadoop 的 <strong>HDFS</strong> 系统使用起来就像是 Linux 的文件系统, 如 <code>hadoop fs -ls /</code> 查看的就是 HDFS 根目录下的列表, 切不要将它们混淆了, HDFS 的根目录可不在 Linux 的根目录 (<code>/</code>) 下, 是两套完全不同的体系.</p>
<p>上传完毕后可以在 Web 界面的文件管理模块可以看到变化:</p>
<p><img alt="" src="http://wiki.smallcpp.cn/static/images/搭建Hadoop分布式实验环境/hadoopfile.png" /></p>
<p><img alt="" src="http://wiki.smallcpp.cn/static/images/搭建Hadoop分布式实验环境/hadoopinfo.png" /></p>
<h2 id="72">7.2. 下载文件</h2>
<p><code>hadoop fs -get /ubuntu-amd64.iso /home/martin/桌面/ubuntu-amd64.iso</code></p>
<p>下载文件到 Linux 系统.</p>
<p>执行命令的时候, 可能会出现提示: <code>WARN hdfs.DFSClient: DFSInputStream has been closed already</code>.</p>
<p>不用管它, apache 也给出了说明:</p>
<p><img alt="" src="http://i61.tinypic.com/344ql4k.jpg" /></p>
<h2 id="73-mr">7.3. 测试 MR 作业</h2>
<p>MR 使用 Java 进行开发, Hadoop 预置了一些测试 MR 作业 (就是一些 jar 包), 它们在: <code>/usr/smallcpp/hadoop-2.7.3/share/hadoop/mapreduce</code> 目录下.</p>
<p>创建一个文件 <code>vim words.txt</code>, 输入内容:</p>
<div class="hlcode"><pre><span class="n">hello</span> <span class="n">tom</span>
<span class="n">hello</span> <span class="n">jerry</span>
<span class="n">hello</span> <span class="n">kitty</span>
<span class="n">hello</span> <span class="n">world</span>
<span class="n">hello</span> <span class="n">martin</span>
</pre></div>


<p><br><br />
所有的 MR 都是执行在 <strong>hdfs</strong> 上的, 所以要先上传文件: <code>hadoop fs -put words.txt /words.txt</code></p>
<p><code>/usr/smallcpp/hadoop-2.7.3/share/hadoop/mapreduce</code> 目录下有个 <code>hadoop-mapreduce-examples-2.7.3.jar</code>, 里面有个 <code>wordcount</code> 方法, 可以用来统计单词个数.</p>
<div class="hlcode"><pre><span class="n">cd</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">smallcpp</span><span class="o">/</span><span class="n">hadoop</span><span class="o">-</span><span class="mf">2.7.3</span><span class="o">/</span><span class="n">share</span><span class="o">/</span><span class="n">hadoop</span><span class="o">/</span><span class="n">mapreduce</span>
<span class="n">hadoop</span> <span class="n">jar</span> <span class="n">hadoop</span><span class="o">-</span><span class="n">mapreduce</span><span class="o">-</span><span class="n">examples</span><span class="o">-</span><span class="mf">2.7.3</span><span class="p">.</span><span class="n">jar</span> <span class="n">wordcount</span> <span class="o">/</span><span class="n">words</span><span class="p">.</span><span class="n">txt</span> <span class="o">/</span><span class="n">result</span>
</pre></div>


<p><br><br />
第一个参数是待统计文件, 第二个参数是保存结果的目录.</p>
<p>执行完毕后, 查看下 hdfs 系统: <code>hadoop fs -ls /</code></p>
<div class="hlcode"><pre><span class="n">Found</span> <span class="mi">4</span> <span class="n">items</span>
<span class="n">drwxr</span><span class="o">-</span><span class="n">xr</span><span class="o">-</span><span class="n">x</span>   <span class="o">-</span> <span class="n">martin</span> <span class="n">supergroup</span>          <span class="mi">0</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="o">:</span><span class="mi">33</span> <span class="o">/</span><span class="n">result</span>
<span class="n">drwxrwx</span><span class="o">---</span>   <span class="o">-</span> <span class="n">martin</span> <span class="n">supergroup</span>          <span class="mi">0</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">16</span> <span class="mi">20</span><span class="o">:</span><span class="mi">27</span> <span class="o">/</span><span class="n">tmp</span>
<span class="o">-</span><span class="n">rw</span><span class="o">-</span><span class="n">r</span><span class="o">--</span><span class="n">r</span><span class="o">--</span>   <span class="mi">2</span> <span class="n">martin</span> <span class="n">supergroup</span> <span class="mi">1485881344</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="o">:</span><span class="mo">01</span> <span class="o">/</span><span class="n">ubuntu</span><span class="o">-</span><span class="n">amd64</span><span class="p">.</span><span class="n">iso</span>
<span class="o">-</span><span class="n">rw</span><span class="o">-</span><span class="n">r</span><span class="o">--</span><span class="n">r</span><span class="o">--</span>   <span class="mi">2</span> <span class="n">martin</span> <span class="n">supergroup</span>         <span class="mi">59</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="o">:</span><span class="mi">30</span> <span class="o">/</span><span class="n">words</span><span class="p">.</span><span class="n">txt</span>
</pre></div>


<p><br><br />
<code>/result</code> 就是刚生成的结果目录, 用 <code>hadoop fs -ls /result</code> 查看下里面的内容:</p>
<div class="hlcode"><pre><span class="n">Found</span> <span class="mi">2</span> <span class="n">items</span>
<span class="o">-</span><span class="n">rw</span><span class="o">-</span><span class="n">r</span><span class="o">--</span><span class="n">r</span><span class="o">--</span>   <span class="mi">2</span> <span class="n">martin</span> <span class="n">supergroup</span>          <span class="mi">0</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="o">:</span><span class="mi">36</span> <span class="o">/</span><span class="n">result</span><span class="o">/</span><span class="n">_SUCCESS</span>
<span class="o">-</span><span class="n">rw</span><span class="o">-</span><span class="n">r</span><span class="o">--</span><span class="n">r</span><span class="o">--</span>   <span class="mi">2</span> <span class="n">martin</span> <span class="n">supergroup</span>         <span class="mi">47</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="o">:</span><span class="mi">36</span> <span class="o">/</span><span class="n">result</span><span class="o">/</span><span class="n">part</span><span class="o">-</span><span class="n">r</span><span class="o">-</span><span class="mo">00000</span>
</pre></div>


<p><br><br />
其中 <code>_SUCCESS</code> 表示 MR 作业执行成功, <code>part-r-00000</code> 为结果文件, 用 <code>hadoop fs -cat /result/part-r-00000</code> 查看下:</p>
<div class="hlcode"><pre><span class="n">hello</span>   <span class="mi">5</span>
<span class="n">jerry</span>   <span class="mi">1</span>
<span class="n">kitty</span>   <span class="mi">1</span>
<span class="n">martin</span>  <span class="mi">1</span>
<span class="n">tom</span>     <span class="mi">1</span>
<span class="n">world</span>   <span class="mi">1</span>
</pre></div>


<p><br><br />
当然也可以直接通过 Web 查看.</p>
<h1 id="references">References</h1>
<p><a href="http://www.powerxing.com/install-hadoop/">Hadoop 安装教程_单机/伪分布式配置_Hadoop2.6.0/Ubuntu14.04</a><br><br />
<a href="http://www.powerxing.com/install-hadoop-cluster/">Hadoop 集群安装配置教程_Hadoop2.6.0/Ubuntu14.04</a></p>
    </div>

    <!-- 多说评论框 start -->
    <div class="ds-thread" data-thread-key="搭建 Hadoop 分布式实验环境" data-title="搭建 Hadoop 分布式实验环境" data-url="http://wiki.smallcpp.cn/Hadoop/搭建 Hadoop 分布式实验环境.html"></div>
    <!-- 多说评论框 end -->
    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
    <script type="text/javascript">
    var duoshuoQuery = {short_name:"smallwiki"};
        (function() {
            var ds = document.createElement('script');
            ds.type = 'text/javascript';ds.async = true;
            ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
            ds.charset = 'UTF-8';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
        })();
    </script>
    <!-- 多说公共JS代码 end -->


        </div>
        <div id="footer">
              <p>
                Copyright © 2012-2017 <a href="http://www.samllcpp.com/" target="_blank">Martin</a> Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
              </p>
        </div>
    </body>
</html>